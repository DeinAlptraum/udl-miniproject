{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d36e1f-532f-4a71-a175-da3d5060be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import gzip\n",
    "import pickle\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7390bcb-1cde-4853-866f-1b3db6ad8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\"\n",
    "\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, val_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_x = np.append(train_set[0], val_set[0], axis = 0)\n",
    "num_features = train_x.shape[1]\n",
    "train_y = np.append(train_set[1], val_set[1], axis = 0)\n",
    "train_set = torch.utils.data.TensorDataset(torch.Tensor(train_x), torch.LongTensor(train_y))\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_set = torch.utils.data.TensorDataset(torch.Tensor(test_set[0]), torch.LongTensor(test_set[1]))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "num_features = train_x.shape[1]\n",
    "num_outputs = train_y.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db91963-11b8-4774-8c1a-33028add865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dataloaders for the training and test datasets with randomly permuted feature columns\n",
    "def generate_permuted_datasets():\n",
    "    with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "        train_set, val_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "    \n",
    "    train_x = np.append(train_set[0], val_set[0], axis = 0)\n",
    "    train_y = np.append(train_set[1], val_set[1], axis = 0)\n",
    "    test_x = test_set[0]\n",
    "    test_y = test_set[1]\n",
    "\n",
    "    # Shuffle feature columns\n",
    "    rand_idxs = np.arange(num_features)\n",
    "    np.random.shuffle(rand_idxs)\n",
    "    train_x = train_x[:,rand_idxs]\n",
    "    test_x = test_x[:,rand_idxs]\n",
    "\n",
    "\n",
    "    train_set = torch.utils.data.TensorDataset(torch.Tensor(train_x), torch.LongTensor(train_y))\n",
    "    test_set = torch.utils.data.TensorDataset(torch.Tensor(test_x), torch.LongTensor(test_y))\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0533b322-177a-474e-a327-01a3778738c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametrization:\n",
    "    def __init__(self, w_mean, log_w_var, b_mean, log_b_var):\n",
    "        self.w_mean = w_mean.to(device)\n",
    "        self.log_w_var = log_w_var.to(device)\n",
    "        self.b_mean = b_mean.to(device)\n",
    "        self.log_b_var = log_b_var.to(device)\n",
    "\n",
    "class VCLLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior: Parametrization):\n",
    "        super(VCLLayer, self).__init__()\n",
    "        self.prior = deepcopy(prior)\n",
    "        self.w_mean = nn.Parameter(prior.w_mean)\n",
    "        self.log_w_var = nn.Parameter(prior.log_w_var)\n",
    "        self.b_mean = nn.Parameter(prior.b_mean)\n",
    "        self.log_b_var = nn.Parameter(prior.log_b_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_eps = torch.randn_like(self.w_mean)\n",
    "        w_std = (0.5*self.log_w_var).exp()\n",
    "        b_eps = torch.randn_like(self.b_mean)\n",
    "        b_std = (0.5*self.log_b_var).exp()\n",
    "        weights = self.w_mean + w_eps * w_std\n",
    "        bias = self.b_mean + b_eps * b_std\n",
    "        return torch.matmul(x, weights) + bias\n",
    "\n",
    "    def update_priors(self):\n",
    "        self.prior = deepcopy(Parametrization(self.w_mean, self.log_w_var, self.b_mean, self.log_b_var))\n",
    "\n",
    "    def restore_from_priors(self):\n",
    "        self.w_mean = deepcopy(self.prior.w_mean)\n",
    "        self.b_mean = deepcopy(self.prior.b_mean)\n",
    "        self.log_w_var = deepcopy(self.prior.log_w_var)\n",
    "        self.log_b_var = deepcopy(self.prior.log_b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3a72ea-3d37-466d-b756-d9cb8b944bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCLNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_priors: list[Parametrization], num_samples, kl_strength):\n",
    "        super(VCLNN, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.kl_strength = kl_strength\n",
    "        self.layers = nn.Sequential(\n",
    "            VCLLayer(input_dim, hidden_dim, layer_priors[0]),\n",
    "            nn.ReLU(),\n",
    "            VCLLayer(hidden_dim, hidden_dim, layer_priors[1]),\n",
    "            nn.ReLU(),\n",
    "            VCLLayer(hidden_dim, output_dim, layer_priors[2])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat([self.num_samples, 1, 1]).flatten(0,1)\n",
    "        return self.layers(x)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        pred_loss = F.cross_entropy(predictions, targets)\n",
    "        # Compute KL divergence\n",
    "        kl_div = 0.0\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(3):\n",
    "            cur_layer = self.layers[l*2]\n",
    "            ############\n",
    "            # Helper to compute KL divergence on just bias/just weights\n",
    "            def _compute_elementary_kl(cur_means, cur_vars, prior_means, prior_vars):\n",
    "                var_div = prior_vars - cur_vars\n",
    "                mean_div = (cur_vars.exp() + (prior_means - cur_means).square()) / prior_vars.exp()\n",
    "                return 0.5 * (mean_div + var_div - 1).sum()\n",
    "            ############\n",
    "            kl_div += _compute_elementary_kl(cur_layer.w_mean, cur_layer.log_w_var, cur_layer.prior.w_mean, cur_layer.prior.log_w_var)\n",
    "            kl_div += _compute_elementary_kl(cur_layer.b_mean, cur_layer.log_b_var, cur_layer.prior.b_mean, cur_layer.prior.log_b_var)\n",
    "\n",
    "        return pred_loss + self.kl_strength*kl_div/(len(targets))\n",
    "\n",
    "    def update_priors(self):\n",
    "        for l in [0,2,4]:\n",
    "            self.layers[l].update_priors()\n",
    "\n",
    "    def restore_from_priors(self):\n",
    "        for l in [0,2,4]:\n",
    "            self.layers[l].restore_from_priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c2fd82-7389-49b9-8236-2bf856f03093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior: Parametrization):\n",
    "        super(BaseLayer, self).__init__()\n",
    "        self.w = nn.Parameter(prior.w_mean)\n",
    "        self.b = nn.Parameter(prior.b_mean)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.w) + self.b\n",
    "\n",
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_priors: list[Parametrization]):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            BaseLayer(input_dim, hidden_dim, layer_priors[0]),\n",
    "            nn.ReLU(),\n",
    "            BaseLayer(hidden_dim, hidden_dim, layer_priors[1]),\n",
    "            nn.ReLU(),\n",
    "            BaseLayer(hidden_dim, output_dim, layer_priors[2])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        return F.cross_entropy(predictions, targets)\n",
    "\n",
    "    def get_weights(self, init_variance):\n",
    "        params = []\n",
    "        for l in [0,2,4]:\n",
    "            cur_l = self.layers[l]\n",
    "            w_m = cur_l.w\n",
    "            b_m = cur_l.b\n",
    "            w_v = torch.zeros(w_m.shape) + init_variance\n",
    "            b_v = torch.zeros(b_m.shape) + init_variance\n",
    "            params.append(Parametrization(w_m, w_v, b_m, b_v))\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce34dbdc-53da-4dec-9794-cd140db41534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights(input_dim, hidden_dim, output_dim, num_layers, init_variance):\n",
    "    params = []\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            in_dim = input_dim\n",
    "        else:\n",
    "            in_dim = hidden_dim\n",
    "        if i == num_layers-1:\n",
    "            out_dim = output_dim\n",
    "        else:\n",
    "            out_dim = hidden_dim\n",
    "\n",
    "        weight_means = torch.zeros(in_dim, out_dim)\n",
    "        weight_variances = torch.zeros(in_dim, out_dim) + init_variance\n",
    "        bias_means = torch.zeros(out_dim)\n",
    "        bias_variances = torch.zeros(out_dim) + init_variance\n",
    "\n",
    "        weights = torch.normal(mean=weight_means, std=0.1)\n",
    "        bias = torch.normal(mean=bias_means, std=0.1)\n",
    "\n",
    "        params.append(Parametrization(weights, weight_variances, bias, bias_variances))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0899b1d-b41f-4990-a207-cb3c2e22e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, epochs, learning_rate=0.001, sampling=False):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            if sampling:\n",
    "                targets = targets.repeat([model.num_samples, 1]).flatten()\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test(model, data_loader, sampling=False):\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    num_samples = 1\n",
    "    if sampling:\n",
    "        num_samples = model.num_samples\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        if sampling:\n",
    "            targets = targets.repeat([model.num_samples, 1]).flatten()\n",
    "        preds = model(inputs)\n",
    "        preds = F.softmax(model(inputs), dim=1)\n",
    "        class_preds = preds.argmax(dim=1)\n",
    "        hits += (class_preds == targets).sum()\n",
    "    return hits/(len(data_loader.dataset) * num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6fdd9a-2bfe-41f4-af58-4472350798d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_coreset(dataset, coreset_size):\n",
    "    perm_idxs = torch.randperm(len(dataset))\n",
    "    coreset_idxs = perm_idxs[:coreset_size]\n",
    "    remainder_idxs = perm_idxs[coreset_size:]\n",
    "    remainder_set = torch.utils.data.TensorDataset(dataset[remainder_idxs][0], dataset[remainder_idxs][1])\n",
    "    core_set = torch.utils.data.TensorDataset(dataset[coreset_idxs][0], dataset[coreset_idxs][1])\n",
    "    return remainder_set, core_set\n",
    "\n",
    "def perform_vcl(num_tasks, num_epochs, coreset_size=0, num_samples=100, init_variance=-6.0, pre_training=True, kl_strength=1.0):\n",
    "    train_set, test_set = generate_permuted_datasets()\n",
    "    test_sets = [test_set]\n",
    "\n",
    "    priors = generate_weights(num_features, 100, num_outputs, 3, init_variance)\n",
    "    if pre_training:\n",
    "        base_model = BaseNN(num_features, 100, num_outputs, layer_priors=priors).to(device)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        train(base_model, train_loader, base_model.loss, epochs=num_epochs)\n",
    "        acc = test(base_model, test_loader)\n",
    "        print(f\"Base model acc: {acc*100:.4}%\")\n",
    "\n",
    "        priors = base_model.get_weights(init_variance)\n",
    "\n",
    "    vclm = VCLNN(num_features, 100, num_outputs, layer_priors=priors, num_samples=num_samples, kl_strength=kl_strength).to(device)\n",
    "\n",
    "    task_results = torch.zeros((num_tasks, num_tasks+1))\n",
    "    coresets = []\n",
    "    for t in range(num_tasks):\n",
    "        if coreset_size > 0:\n",
    "            train_set, coreset = split_coreset(train_set, coreset_size)\n",
    "            coresets.append(coreset)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        train(vclm, train_loader, vclm.loss, epochs=num_epochs, sampling=True)\n",
    "        accs = torch.zeros((num_tasks+1,))\n",
    "\n",
    "        # First, train model on coreset if used\n",
    "        vclm.update_priors()\n",
    "        if coreset_size > 0:\n",
    "            coreset_dataset = torch.utils.data.ConcatDataset(coresets)\n",
    "            coreset_loader = torch.utils.data.DataLoader(coreset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            train(vclm, coreset_loader, vclm.loss, epochs=num_epochs, sampling=True)\n",
    "\n",
    "        # Test all tasks together\n",
    "        combined_test_set = torch.utils.data.ConcatDataset(test_sets)\n",
    "        test_loader = torch.utils.data.DataLoader(combined_test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        accs[0] = test(vclm, test_loader, sampling=True).item()*100.0\n",
    "        # Test each task one by one\n",
    "        for i, test_set in enumerate(test_sets):\n",
    "            test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            accs[i+1] = test(vclm, test_loader, sampling=True).item()*100.0\n",
    "        print(f\"Task {t} accuracies: {accs}\")\n",
    "        task_results[t] = accs\n",
    "        # After testing, restore model weights from the priors to those not trained via coresets\n",
    "        vclm.restore_from_priors()\n",
    "\n",
    "        train_set, test_set = generate_permuted_datasets()\n",
    "        test_sets.append(test_set)\n",
    "\n",
    "    return task_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c1de2d-c329-47ff-af3e-ce2154094e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def average_runs(num_runs, num_tasks, num_epochs, coreset_size, num_samples=100, init_variance=-6.0, pre_training=True, kl_strength=1.0):\n",
    "    vcl_results = None\n",
    "    for _ in range(num_runs):\n",
    "        res = perform_vcl(num_tasks, num_epochs, coreset_size, num_samples=num_samples, pre_training=pre_training, init_variance=init_variance, kl_strength=kl_strength)\n",
    "        if vcl_results is None:\n",
    "            vcl_results = res[None,:]\n",
    "        else:\n",
    "            vcl_results = torch.cat((vcl_results, res[None,:]))\n",
    "\n",
    "    return vcl_results[:,:,0].mean(dim=0), vcl_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5564ef0b-ac0b-42aa-835f-1fc30c545625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a single benchmark with the given settings\n",
    "acc, all_res = average_runs(10, 10, 30, 200, num_samples=100, pre_training=True, init_variance=-6.0, kl_strength=1.0)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
